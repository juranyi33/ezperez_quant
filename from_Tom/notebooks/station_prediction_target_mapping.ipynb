{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib \n",
    "matplotlib.use('Agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "\n",
    "char = \"\\\\\"\n",
    "columns = [\"date\", \"tavg\", \"tmin\", \"tmax\", \"prcp\"]\n",
    "t_columns = [i+\"_x\" for i in columns]\n",
    "s_columns = [i+\"_y\" for i in columns]\n",
    "op_into = False\n",
    "empty_pred_target = \"GqIUVenONyZikTIz\"\n",
    "save_plots = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n",
      "197\n"
     ]
    }
   ],
   "source": [
    "# get pathsfor daily and target\n",
    "\n",
    "min_daily_weather_paths = [f for f in glob(\"../data/weather/minnesota_daily/*.csv\")]\n",
    "pred_targets_paths = [f for f in glob(\"../data/weather/prediction_targets_daily/*.csv\") if empty_pred_target not in f]\n",
    "\n",
    "print(len(min_daily_weather_paths))\n",
    "print(len(pred_targets_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in all daily weather csvs\n",
    "\n",
    "min_daily_weather_dict = {}\n",
    "\n",
    "for path in min_daily_weather_paths:\n",
    "\n",
    "    station_code = path.split(char)[-1].split(\".\")[0]\n",
    "\n",
    "    min_daily_weather_dict[station_code] = pd.read_csv(filepath_or_buffer=path, names=columns, index_col=\"date\")\n",
    "    \n",
    "    if op_into:\n",
    "        print(f\"{station_code}:\\n\\tfrom: {min_daily_weather_dict[station_code].index[0]}\\n\\tto: {min_daily_weather_dict[station_code].index[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate weights for averaging features\n",
    "# weight depends on nan values in each feature\n",
    "# prediction targets are left out to prevent data leakage\n",
    "\n",
    "nan_counts = np.zeros((len(min_daily_weather_paths), 4))\n",
    "\n",
    "for i, path in enumerate(min_daily_weather_paths):\n",
    "    num_nans = pd.read_csv(filepath_or_buffer=path, names=columns, index_col=\"date\").isna().sum()\n",
    "    nan_counts[i] = np.array(num_nans)\n",
    "\n",
    "weights = np.sum(a=nan_counts, axis=0)\n",
    "weights = 1 - (weights/np.sum(a=weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GTpYgkFKgvhUVIdW and 9NRIJ have no matching dates.\n",
      "GTpYgkFKgvhUVIdW and KD390 have no matching dates.\n",
      "GTpYgkFKgvhUVIdW and KRYM0 have no matching dates.\n",
      "GTpYgkFKgvhUVIdW and KY490 have no matching dates.\n",
      "GTpYgkFKgvhUVIdW and KY630 have no matching dates.\n",
      "GTpYgkFKgvhUVIdW and P6529 have no matching dates.\n",
      "GTpYgkFKgvhUVIdW and UYB6K have no matching dates.\n",
      "GTpYgkFKgvhUVIdW and X9FED have no matching dates.\n",
      "GTpYgkFKgvhUVIdW and Z7ZOG have no matching dates.\n",
      "IuUAGHGofIeJLHnH and 9NRIJ have no matching dates.\n",
      "IuUAGHGofIeJLHnH and KD390 have no matching dates.\n",
      "IuUAGHGofIeJLHnH and KY490 have no matching dates.\n",
      "IuUAGHGofIeJLHnH and P6529 have no matching dates.\n",
      "IuUAGHGofIeJLHnH and UYB6K have no matching dates.\n",
      "IuUAGHGofIeJLHnH and X9FED have no matching dates.\n",
      "IuUAGHGofIeJLHnH and Z7ZOG have no matching dates.\n",
      "mMNhXEjWMwSvpLph and 9NRIJ have no matching dates.\n",
      "mMNhXEjWMwSvpLph and P6529 have no matching dates.\n",
      "mMNhXEjWMwSvpLph and UYB6K have no matching dates.\n",
      "mMNhXEjWMwSvpLph and X9FED have no matching dates.\n",
      "mMNhXEjWMwSvpLph and Z7ZOG have no matching dates.\n",
      "YRbFthxdZAHOIvEq and 9NRIJ have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KACQ0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KADC0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KAEL0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KAIT0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KAQP0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KAUM0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KBBB0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KBDE0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KBDH0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KBFW0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KBRD0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KCBG0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KCDD0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KCFE0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KCKC0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KCKN0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KCNB0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KCOQ0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KCQM0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KD390 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KDTL0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KDXX0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KDYT0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KELO0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KETH0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KEVM0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KFBL0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KFCM0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KFFM0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KFGN0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KFKA0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KFOZ0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KFRM0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KFSE0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KGDB0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KGHW0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KGNA0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KGPZ0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KGYL0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KHCD0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KHCO0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KHZX0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KJKJ0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KJMR0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KJYG0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KLJF0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KLVN0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KLXL0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KLYV0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KMGG0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KMIC0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KMJQ0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KMKT0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KMML0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KMOX0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KMVE0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KMWM0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KMZH0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KONA0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KORB0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KOTG0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KOVL0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KOWA0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KPEX0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KPKD0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KPNM0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KPQN0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KPWC0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KRGK0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KROS0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KRRT0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KRYM0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KSAZ0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KSGS0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KSTP0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KSYN0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KTKC0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KTOB0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KTWM0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KULM0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KVVV0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KXVG0 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KY490 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and KY630 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and P6529 have no matching dates.\n",
      "YRbFthxdZAHOIvEq and UYB6K have no matching dates.\n",
      "YRbFthxdZAHOIvEq and X9FED have no matching dates.\n",
      "YRbFthxdZAHOIvEq and Z7ZOG have no matching dates.\n"
     ]
    }
   ],
   "source": [
    "# read sequentally all prediction targets and compare with all station data\n",
    "\n",
    "distances = {\"Prediction Target\": [],\n",
    "             \"Station\": [],\n",
    "             \"L2 average distance\": [],\n",
    "             \"L2 weighted distance\": []}\n",
    "\n",
    "for i, path in enumerate(pred_targets_paths):\n",
    "\n",
    "    prediction_code = path.split(char)[-1].split(\".\")[0]\n",
    "\n",
    "    df_target = pd.read_csv(filepath_or_buffer=path, names=columns, index_col=\"date\")\n",
    "\n",
    "    for station in min_daily_weather_dict.keys():\n",
    "        \n",
    "        # 1, check date correspondence\n",
    "        tmp = pd.merge(left=df_target, right=min_daily_weather_dict[station], left_index=True, right_index=True, how=\"inner\")\n",
    "\n",
    "        if op_into:\n",
    "            print(tmp)\n",
    "\n",
    "        if tmp.empty:\n",
    "            print(f\"{prediction_code} and {station} have no matching dates.\")\n",
    "\n",
    "            distances[\"Prediction Target\"].append(prediction_code)\n",
    "            distances[\"Station\"].append(station)\n",
    "            distances[\"L2 average distance\"].append(np.inf)\n",
    "            distances[\"L2 weighted distance\"].append(np.inf)\n",
    "\n",
    "        else:\n",
    "            # 2, calculate average L2 distance between all measured modalities\n",
    "            dist_t_avg = (tmp[\"tavg_x\"] - tmp[\"tavg_y\"])**2\n",
    "            dist_t_min = (tmp[\"tmin_x\"] - tmp[\"tmin_y\"])**2\n",
    "            dist_t_max = (tmp[\"tmax_x\"] - tmp[\"tmax_y\"])**2\n",
    "            dist_t_prcp = (tmp[\"prcp_x\"] - tmp[\"prcp_y\"])**2\n",
    "\n",
    "            temp = np.nan_to_num(np.array([dist_t_avg, dist_t_min, dist_t_max, dist_t_prcp]), 0.0)\n",
    "\n",
    "            avg_distance = np.nanmean(temp)\n",
    "            weighted_distance = np.nanmean(weights*temp.T)\n",
    "\n",
    "            distances[\"Prediction Target\"].append(prediction_code)\n",
    "            distances[\"Station\"].append(station)\n",
    "            distances[\"L2 average distance\"].append(avg_distance)\n",
    "            distances[\"L2 weighted distance\"].append(weighted_distance)\n",
    "\n",
    "df_distances = pd.DataFrame(data=distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a pandas DataFrame called df_distances\n",
    "df_grouped = df_distances.groupby('Prediction Target')\n",
    "df_sorted = df_grouped.apply(lambda x: x.sort_values('L2 weighted distance', ascending=True))\n",
    "\n",
    "# Reset the index of the resulting DataFrame\n",
    "df_result = df_sorted.reset_index(drop=True)\n",
    "df_result.to_csv(path_or_buf=\"../data/intermedier/prediction_target_station_distances_L2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prediction Target</th>\n",
       "      <th>Station</th>\n",
       "      <th>L2 average distance</th>\n",
       "      <th>L2 weighted distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>ACAvNTuEuFWcmwms</td>\n",
       "      <td>KROS0</td>\n",
       "      <td>4.744662</td>\n",
       "      <td>3.896683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843</th>\n",
       "      <td>AWUpGSoLMOQdgmLf</td>\n",
       "      <td>KONA0</td>\n",
       "      <td>11.308500</td>\n",
       "      <td>9.141236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>AmezwjsXKaggSICV</td>\n",
       "      <td>KCKN0</td>\n",
       "      <td>14.532802</td>\n",
       "      <td>13.048470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>AvjrUfvxwfPdFfLD</td>\n",
       "      <td>KETH0</td>\n",
       "      <td>4.996847</td>\n",
       "      <td>2.832416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>BLEPTCvUWUfqXlBZ</td>\n",
       "      <td>KCNB0</td>\n",
       "      <td>10.479220</td>\n",
       "      <td>9.041338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18885</th>\n",
       "      <td>zNYjRZijEySKjImp</td>\n",
       "      <td>KONA0</td>\n",
       "      <td>8.577518</td>\n",
       "      <td>7.054493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18982</th>\n",
       "      <td>zThgIIrssGevRmsP</td>\n",
       "      <td>KONA0</td>\n",
       "      <td>19.038213</td>\n",
       "      <td>10.533649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18620</th>\n",
       "      <td>zhXchzMIBxaWiYPf</td>\n",
       "      <td>P6529</td>\n",
       "      <td>14.312543</td>\n",
       "      <td>8.441644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18799</th>\n",
       "      <td>zkNJkuOxjgWDUWIi</td>\n",
       "      <td>KROS0</td>\n",
       "      <td>2.338327</td>\n",
       "      <td>1.913806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19079</th>\n",
       "      <td>zuWktVTnYICcsVFr</td>\n",
       "      <td>KONA0</td>\n",
       "      <td>16.349041</td>\n",
       "      <td>12.829320</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>197 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Prediction Target Station  L2 average distance  L2 weighted distance\n",
       "78     ACAvNTuEuFWcmwms   KROS0             4.744662              3.896683\n",
       "843    AWUpGSoLMOQdgmLf   KONA0            11.308500              9.141236\n",
       "411    AmezwjsXKaggSICV   KCKN0            14.532802             13.048470\n",
       "711    AvjrUfvxwfPdFfLD   KETH0             4.996847              2.832416\n",
       "1091   BLEPTCvUWUfqXlBZ   KCNB0            10.479220              9.041338\n",
       "...                 ...     ...                  ...                   ...\n",
       "18885  zNYjRZijEySKjImp   KONA0             8.577518              7.054493\n",
       "18982  zThgIIrssGevRmsP   KONA0            19.038213             10.533649\n",
       "18620  zhXchzMIBxaWiYPf   P6529            14.312543              8.441644\n",
       "18799  zkNJkuOxjgWDUWIi   KROS0             2.338327              1.913806\n",
       "19079  zuWktVTnYICcsVFr   KONA0            16.349041             12.829320\n",
       "\n",
       "[197 rows x 4 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = df_distances.groupby('Prediction Target')['L2 weighted distance'].idxmin()\n",
    "result = df_distances.loc[idx]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prediction Target</th>\n",
       "      <th>Station</th>\n",
       "      <th>L2 average distance</th>\n",
       "      <th>L2 weighted distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3435</th>\n",
       "      <td>DfizCymEXgwIjnki</td>\n",
       "      <td>KFRM0</td>\n",
       "      <td>2.019714</td>\n",
       "      <td>1.643573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000</th>\n",
       "      <td>DusycBGfBqLIQNFf</td>\n",
       "      <td>KCKN0</td>\n",
       "      <td>2.282793</td>\n",
       "      <td>1.800002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1039</th>\n",
       "      <td>bJfBssUbPvnDtmOe</td>\n",
       "      <td>KOTG0</td>\n",
       "      <td>2.680624</td>\n",
       "      <td>1.862679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18799</th>\n",
       "      <td>zkNJkuOxjgWDUWIi</td>\n",
       "      <td>KROS0</td>\n",
       "      <td>2.338327</td>\n",
       "      <td>1.913806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8708</th>\n",
       "      <td>KVYsyLIvhwwqgSun</td>\n",
       "      <td>KPQN0</td>\n",
       "      <td>2.561615</td>\n",
       "      <td>2.052540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5410</th>\n",
       "      <td>GTpYgkFKgvhUVIdW</td>\n",
       "      <td>KPQN0</td>\n",
       "      <td>21.535062</td>\n",
       "      <td>20.257330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15710</th>\n",
       "      <td>VasKmVkDnzxJzhep</td>\n",
       "      <td>P6529</td>\n",
       "      <td>28.474237</td>\n",
       "      <td>20.443796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12958</th>\n",
       "      <td>RfnwXzFAVgaCkooQ</td>\n",
       "      <td>KLYV0</td>\n",
       "      <td>25.811087</td>\n",
       "      <td>22.553979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18236</th>\n",
       "      <td>YRbFthxdZAHOIvEq</td>\n",
       "      <td>72644</td>\n",
       "      <td>26.685666</td>\n",
       "      <td>26.254729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>aNBbBQsLuzazxQDL</td>\n",
       "      <td>72658</td>\n",
       "      <td>59.282703</td>\n",
       "      <td>33.909155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>197 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Prediction Target Station  L2 average distance  L2 weighted distance\n",
       "3435   DfizCymEXgwIjnki   KFRM0             2.019714              1.643573\n",
       "4000   DusycBGfBqLIQNFf   KCKN0             2.282793              1.800002\n",
       "1039   bJfBssUbPvnDtmOe   KOTG0             2.680624              1.862679\n",
       "18799  zkNJkuOxjgWDUWIi   KROS0             2.338327              1.913806\n",
       "8708   KVYsyLIvhwwqgSun   KPQN0             2.561615              2.052540\n",
       "...                 ...     ...                  ...                   ...\n",
       "5410   GTpYgkFKgvhUVIdW   KPQN0            21.535062             20.257330\n",
       "15710  VasKmVkDnzxJzhep   P6529            28.474237             20.443796\n",
       "12958  RfnwXzFAVgaCkooQ   KLYV0            25.811087             22.553979\n",
       "18236  YRbFthxdZAHOIvEq   72644            26.685666             26.254729\n",
       "487    aNBbBQsLuzazxQDL   72658            59.282703             33.909155\n",
       "\n",
       "[197 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.sort_values(\"L2 weighted distance\", ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load station - county mapper\n",
    "\n",
    "station_county_map = pd.read_csv(filepath_or_buffer=\"../data/intermedier/stations_county_affiliation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results in csv\n",
    "\n",
    "final = pd.merge(left=result, right=station_county_map, left_on=\"Station\", right_on=\"Station\")\n",
    "final[[\"Prediction Target\", \"Station\", \"County\"]].to_csv(path_or_buf=\"../data/intermedier/prediction_target_station_matching.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot closest parts\n",
    "\n",
    "color_s = \"#0B2447\"\n",
    "color_t = \"#A5D7E8\"\n",
    "\n",
    "if save_plots:\n",
    "\n",
    "    for pair in result.iterrows():\n",
    "\n",
    "        station = pair[1][\"Station\"]\n",
    "        target = pair[1][\"Prediction Target\"]\n",
    "\n",
    "        statioin_path = [s for s in min_daily_weather_paths if station in s][0]\n",
    "        target_path = [s for s in pred_targets_paths if target in s][0]\n",
    "\n",
    "        df_station = pd.read_csv(filepath_or_buffer=statioin_path, names=columns, index_col=\"date\")\n",
    "        df_target = pd.read_csv(filepath_or_buffer=target_path, names=columns, index_col=\"date\")\n",
    "\n",
    "        tmp = pd.merge(left=df_station, right=df_target, left_index=True, right_index=True, how=\"inner\", suffixes=[\"_station\", \"_target\"])\n",
    "\n",
    "        corr = tmp.corr()\n",
    "\n",
    "        with plt.ioff():\n",
    "        \n",
    "            title = f\"{station} - {target}: {station_county_map[station_county_map['Station'] == station]['County'].values[0]}\"\n",
    "\n",
    "            fig, ax = plt.subplots(ncols=1, nrows=4, figsize=(18, 8))\n",
    "\n",
    "            ax[0].set_title(title)\n",
    "\n",
    "            ax[0].plot(tmp.index, tmp[\"tavg_station\"], label=\"tavg_station\", linewidth=2, alpha=0.75, color=color_s)\n",
    "            ax[0].plot(tmp.index, tmp[\"tavg_target\"] , label=\"tavg_target\", linewidth=2, alpha=0.75, color=color_t)\n",
    "            ax[0].text(0, 0, f\"corr: {corr['tavg_station']['tavg_target']}\", fontsize=12)\n",
    "            ax[0].set_xticks(tmp.index[::int(len(tmp.index)/10)])\n",
    "            ax[0].set_ylabel(\"°C\")\n",
    "            ax[0].legend()\n",
    "\n",
    "            ax[1].plot(tmp.index, tmp[\"tmin_station\"], label=\"tmin_station\", linewidth=2, alpha=0.75, color=color_s)\n",
    "            ax[1].plot(tmp.index, tmp[\"tmin_target\"] , label=\"tmin_target\", linewidth=2, alpha=0.75, color=color_t)\n",
    "            ax[1].text(0, 0, f\"corr: {corr['tmin_station']['tmin_target']}\", fontsize=12)\n",
    "            ax[1].set_xticks(tmp.index[::int(len(tmp.index)/10)])\n",
    "            ax[1].set_ylabel(\"°C\")\n",
    "            ax[1].legend()\n",
    "\n",
    "            ax[2].plot(tmp.index, tmp[\"tmax_station\"], label=\"tmax_station\", linewidth=2, alpha=0.75, color=color_s)\n",
    "            ax[2].plot(tmp.index, tmp[\"tmax_target\"] , label=\"tmax_target\", linewidth=2, alpha=0.75, color=color_t)\n",
    "            ax[2].text(0, 0, f\"corr: {corr['tmax_target']['tmax_target']}\", fontsize=12)\n",
    "            ax[2].set_xticks(tmp.index[::int(len(tmp.index)/10)])\n",
    "            ax[2].set_ylabel(\"°C\")\n",
    "            ax[2].legend()\n",
    "\n",
    "            ax[3].plot(tmp.index, tmp[\"prcp_station\"], label=\"prcp_station\", linewidth=2, alpha=0.75, color=color_s)\n",
    "            ax[3].plot(tmp.index, tmp[\"prcp_target\"] , label=\"prcp_target\", linewidth=2, alpha=0.75, color=color_t)\n",
    "            ax[3].text(0, 0, f\"corr: {corr['prcp_target']['prcp_target']}\", fontsize=12)\n",
    "            ax[3].set_xticks(tmp.index[::int(len(tmp.index)/10)])\n",
    "            ax[3].set_ylabel(\"mm\")\n",
    "            ax[3].legend()\n",
    "\n",
    "            plt.savefig(fname=f\"../data/intermedier/distance/{station}_{target}.png\",\n",
    "                        bbox_inches=\"tight\",\n",
    "                        pad_inches = 0.1,\n",
    "                        dpi=100)\n",
    "            \n",
    "            plt.close()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
